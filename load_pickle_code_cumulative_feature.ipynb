{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. mon_standard.pkl > array code\n",
        "\n"
      ],
      "metadata": {
        "id": "6a7b3a0l2ep4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np # ******추가된 부분*******\n",
        "\n",
        "USE_SUBLABEL = False\n",
        "URL_PER_SITE = 10\n",
        "TOTAL_URLS   = 950\n",
        "\n",
        "# Load the pickle file\n",
        "print(\"Loading datafile...\")\n",
        "with open(\"/content/mon_standard.pkl\", 'rb') as fi: # Path to mon_standard.pkl in Colab\n",
        "    data = pickle.load(fi)\n",
        "\n",
        "X1 = [] # Array to store instances (timestamps) - 19,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
        "X2 = [] # Array to store instances (direction*size) - size information\n",
        "X_mon = [] # ******추가된 부분*******\n",
        "y = [] # Array to store the site of each instance - 19,000 instances, e.g., [0, 0, 0, 0, 0, 0, ..., 94, 94, 94, 94, 94]\n",
        "\n",
        "# Differentiate instances and sites, and store them in the respective x and y arrays\n",
        "# x array (direction*timestamp), y array (site label)\n",
        "for i in range(TOTAL_URLS):\n",
        "    if USE_SUBLABEL:\n",
        "        label = i\n",
        "    else:\n",
        "        label = i // URL_PER_SITE # Calculate which site's URL the current URL being processed belongs to and set that value as the label. Thus, URLs fetched from the same site are labeled identically.\n",
        "    for sample in data[i]:\n",
        "        size_seq = []\n",
        "        time_seq = []\n",
        "        for c in sample:\n",
        "            dr = 1 if c > 0 else -1\n",
        "            time_seq.append(abs(c))\n",
        "            size_seq.append(dr * 512)\n",
        "        X1.append(time_seq)\n",
        "        X2.append(size_seq)\n",
        "        y.append(label)\n",
        "        # ******추가된 부분*******\n",
        "        cumulative_seq = np.cumsum(size_seq)\n",
        "        X_mon.append(cumulative_seq)\n",
        "\n",
        "size = len(y)\n",
        "\n",
        "print(f'Total samples: {size}') # Output: 19000\n",
        "print(f\"Cumulative seq feature samples(monitored): {len(X_mon)}\") # ******추가된 부분*******\n"
      ],
      "metadata": {
        "id": "OPRrp91BKY6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1067ec53-dc3b-4770-dc23-55533bebb93e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datafile...\n",
            "Total samples: 19000\n",
            "Processed 19000 monitored samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. unmon_standard10.pkl > array code"
      ],
      "metadata": {
        "id": "yz5mat0w2dJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np # ******추가된 부분*******\n",
        "\n",
        "TOTAL_URLS = 10000  # total number in the dataset\n",
        "\n",
        "# Load 10,000 unmon pickle file\n",
        "print(\"Loading datafile...\")\n",
        "with open('/content/unmon_standard10.pkl', 'rb') as f:  # Path to unmon_standard10.pkl in Colab\n",
        "    x = pickle.load(f)\n",
        "\n",
        "size = len(x)\n",
        "print(f'Total samples: {size}')\n",
        "\n",
        "X1 = [] # Array to store instances (timestamps) - 10,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
        "X2 = [] # Array to store instances (direction*size) - size information\n",
        "X_unmon = [] # ******추가된 부분*******\n",
        "\n",
        "for i in range(TOTAL_URLS):\n",
        "    size_seq = []\n",
        "    time_seq = []\n",
        "    for c in x[i]:\n",
        "        dr = 1 if c > 0 else -1\n",
        "        time_seq.append(abs(c))\n",
        "        size_seq.append(dr * 512) # In the pickle file, there is no size information, so the conversion code is set to multiply by 512 uniformly.\n",
        "    X1.append(time_seq)\n",
        "    X2.append(size_seq)\n",
        "    # ******추가된 부분*******\n",
        "    cumulative_seq = np.cumsum(size_seq)\n",
        "    X_unmon.append(cumulative_seq)\n",
        "\n",
        "print(len(X1)) # Print the length of X1\n",
        "print(f\"Cumulative seq feature samples(unmonitored): {len(X_unmon)}\") # ******추가된 부분*******\n",
        "\n"
      ],
      "metadata": {
        "id": "ayrcJ7gsLlW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2f88fd-64dd-449c-fab5-e30ec8e308bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datafile...\n",
            "Total samples: 10000\n",
            "10000\n",
            "Processed 10000 unmonitored samples.\n"
          ]
        }
      ]
    }
  ]
}